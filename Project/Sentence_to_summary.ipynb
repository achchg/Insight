{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim, string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "import pickle\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    \n",
    "    # replace ; keeps the sentence structure\n",
    "    text = text.replace(\" ; \", \"\\n\")\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # get rid of punctuation\n",
    "    text = text.translate(table)\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \". \").replace(\"\\r\", \".\")\n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def calc_score(ref, comp, debug=False):\n",
    "    '''gives the number of items in ref that is also found in comp'''\n",
    "    ## check if it is a list of strings\n",
    "    if not isinstance(ref, list):\n",
    "        ref = str(ref).split()\n",
    "    if not isinstance(comp, list):\n",
    "        comp = str(comp).split()\n",
    "        \n",
    "    s_ref = set(ref)\n",
    "    s_comp = set(comp)\n",
    "    s_inter = s_comp.intersection(s_ref)\n",
    "    if debug:\n",
    "        print(s_ref, len(s_ref))\n",
    "        print(s_comp)\n",
    "        print(s_inter, len(s_inter))\n",
    "    if len(s_ref) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(s_inter)/len(s_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\", \"\\n\", \"\", \" \", \"\\n\\n\", \"npr\"]\n",
    "\n",
    "def lemming(data, keeptype=[\"NOUN\", \"PROPN\", \"NUM\", \"ADJ\", \"ADV\"], doalpha=True, dostop=True):\n",
    "    tokens = []\n",
    "    for tok in data:\n",
    "        \n",
    "        # stoplist the tokens\n",
    "        if dostop:\n",
    "            if tok.text not in STOPLIST:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "            ##check if the token is stopword\n",
    "            if not tok.is_stop:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # stoplist symbols\n",
    "        if tok.text not in SYMBOLS:\n",
    "            pass\n",
    "        else: \n",
    "            continue\n",
    "        \n",
    "        ##check if the token is alpha\n",
    "        if doalpha:\n",
    "            if tok.is_alpha:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        ##check if the token is noun\n",
    "        if len(keeptype) > 1:\n",
    "            if tok.pos_ in keeptype:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # lemmatize\n",
    "        if tok.lemma_ != \"-PRON-\" :\n",
    "            tokens.append(tok.lemma_.lower().strip())\n",
    "        else:\n",
    "            tokens.append(tok.lower_)\n",
    "    \n",
    "    # remove large strings of whitespace\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=1,                        # minimum reqd occurences of a word \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'info.pkl', 'rb') as f:\n",
    "    alldic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token = []\n",
    "all_token_alltype = []\n",
    "all_sumtoken = []\n",
    "all_gensim = []\n",
    "N = 419 ##304 NPR, 96 conv, 419 story\n",
    "path = 'Data/NPR_story/'\n",
    "#for i in tqdm(range(N)):\n",
    "info_dic = {}\n",
    "for i in range(1):\n",
    "    test_text = []\n",
    "    test_sumtext   = []\n",
    "    info_text = []\n",
    "    test_scores = []\n",
    "    test_lemlength = []\n",
    "    test_length = []\n",
    "    test_timelength = []\n",
    "    test_maxvolume = []\n",
    "    test_avevolume = []\n",
    "    with open(path + str(i) + '_trans.txt', 'r') as myfile:\n",
    "        test_text = cleanText(myfile.read()).split(\".\")\n",
    "    with open(path + str(i) + '.txt', 'r') as myfile:\n",
    "        test_sumtext = cleanText(myfile.read()).split(\".\")[0]\n",
    "   \n",
    "    for j in range(len(test_text)):\n",
    "        info_text.append(mydic[path + str(i) + '_' + str(j) + '.wav'])\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'] = {}\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'].update({'text': test_text[j]})\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'].update({'sumtext': test_sumtext})\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'].update({'sumtext': test_sumtext})\n",
    "    #print(test_sumtext[-1])\n",
    "    #print(info_text[-1])\n",
    "    \n",
    "    ## summ tocken\n",
    "    all_sumtoken.append(lemming(nlp(test_sumtext), doalpha=True, dostop=True, keeptype=[]))\n",
    "    #print(all_sumtoken[-1])\n",
    "    \n",
    "    for j in range(len(test_text)):\n",
    "        temp_text = test_text[j]\n",
    "        print(temp_text.split(\" \"))\n",
    "        test_token = lemming(nlp(\"\".join(temp_text)), doalpha=True, dostop=True, keeptype=[])\n",
    "        test_scores.append(calc_score(test_token, all_sumtoken[-1]))\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'].update({'score': calc_score(test_token, all_sumtoken[-1])})\n",
    "        test_lemlength.append(len(test_token))\n",
    "        test_length.append(len(temp_text.split(\" \")))\n",
    "        test_timelength.append(info_text[j]['duration'])\n",
    "        test_maxvolume.append(info_text[j]['max_dBFS'])\n",
    "        test_avevolume.append(info_text[j]['dBFS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dic = pd.DataFrame(info_dic).transpose()\n",
    "temp_dic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "#plt.scatter(test_maxvolume, test_scores)\n",
    "plt.scatter(np.array(test_maxvolume)/np.array(test_avevolume), test_scores)\n",
    "#plt.scatter(np.array(test_timelength)/np.array(test_length), test_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text[np.argmax(np.array(test_maxvolume)/np.array(test_avevolume))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(alldic).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'info_1.pkl', 'rb') as f:\n",
    "    tempdic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(tempdic)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
