{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim, string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "import pickle\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    \n",
    "    # replace ;\n",
    "    text = text.replace(\" ; \", \"\\n\")\n",
    "    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # get rid of punctuation\n",
    "    text = text.translate(table)\n",
    "    \n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \". \").replace(\"\\r\", \".\")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    \n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def calc_score(ref, comp, debug=False):\n",
    "    '''gives the number of items in ref that is also found in comp'''\n",
    "    ## check if it is a list of strings\n",
    "    if not isinstance(ref, list):\n",
    "        ref = str(ref).split()\n",
    "    if not isinstance(comp, list):\n",
    "        comp = str(comp).split()\n",
    "        \n",
    "    s_ref = set(ref)\n",
    "    s_comp = set(comp)\n",
    "    s_inter = s_comp.intersection(s_ref)\n",
    "    if debug:\n",
    "        print(s_ref, len(s_ref))\n",
    "        print(s_comp)\n",
    "        print(s_inter, len(s_inter))\n",
    "    if len(s_ref) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(s_inter)/len(s_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\", \"\\n\", \"\", \" \", \"\\n\\n\", \"npr\"]\n",
    "\n",
    "def lemming(data, keeptype=[\"NOUN\", \"PROPN\", \"NUM\", \"ADJ\", \"ADV\"], doalpha=True, dostop=True):\n",
    "    tokens = []\n",
    "    for tok in data:\n",
    "        \n",
    "        # stoplist the tokens\n",
    "        if dostop:\n",
    "            if tok.text not in STOPLIST:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "            ##check if the token is stopword\n",
    "            if not tok.is_stop:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # stoplist symbols\n",
    "        if tok.text not in SYMBOLS:\n",
    "            pass\n",
    "        else: \n",
    "            continue\n",
    "        \n",
    "        ##check if the token is alpha\n",
    "        if doalpha:\n",
    "            if tok.is_alpha:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        ##check if the token is noun\n",
    "        if len(keeptype) > 1:\n",
    "            if tok.pos_ in keeptype:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # lemmatize\n",
    "        if tok.lemma_ != \"-PRON-\" :\n",
    "            tokens.append(tok.lemma_.lower().strip())\n",
    "        else:\n",
    "            tokens.append(tok.lower_)\n",
    "    \n",
    "    # remove large strings of whitespace\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=1,                        # minimum reqd occurences of a word \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'info.pkl', 'rb') as f:\n",
    "    alldic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'uh']\n",
      "['', 'then', 'its', 'friday', 'so', 'its', 'time', 'for', 'storing', 'or']\n",
      "['', 'pamela', 'make', 'any', 'dream', 'that', 'she', 'grew', 'up', 'get', 'married', 'and', 'have', 'kids', 'but', 'it', 'didnt', 'quite', 'work', 'out', 'that', 'way']\n",
      "['', 'she', 'wound', 'up', 'raising', 'her', 'daughter', 'courtney', 'on', 'her', 'own']\n",
      "['', 'so', 'what', 'do', 'you', 'remember', 'each', 'survive', 'tell', 'you', 'cut', 'huh']\n",
      "['', 'that', 'his', 'name', 'is', 'charles', 'and', 'here', 'is', 'my', 'age']\n",
      "['', 'and', 'he', 'has', 'the', 'privilege', 'of', 'seven', 'to', 'work', 'out']\n",
      "['', 'and', 'i', 'just', 'really', 'distinctly', 'remember', 'natalie', 'the', 'courtney', 'was', 'right', 'to', 'be', 'suspicious', 'when', 'she', 'was', 'sixteen', 'she', 'learned', 'that', 'her', 'mom']\n",
      "['', 'actually', 'conceived', 'through', 'an', 'anonymous', 'sperm', 'donation']\n",
      "['', 'ten', 'years', 'they', 'are', 'courting', 'discovered', 'during', 'online', 'dna', 'test']\n",
      "['', 'but', 'yet', 'half', 'sister']\n",
      "['', 'alexander', 'sand']\n",
      "['', 'simon', 'says', 'you', 'know', 'as', 'again', 'chance', 'to', 'you']\n",
      "['', 'having', 'two', 'and', 'a', 'father', 'and']\n",
      "['', 'when', 'they', 'got', 'your', 'message']\n",
      "['', 'and', 'since', 'then', 'crying', 'and', 'said']\n",
      "['', 'hearing', 'your', 'voice', 'for', 'the', 'first', 'time', 'wines']\n",
      "['', 'amazing']\n",
      "['', 'its', 'a', 'really', 'its']\n",
      "['', 'now', 'interest', 'for', 'siblings']\n",
      "['', 'but', 'terminating', 'air', 'was', 'jammed']\n",
      "['', 'and', 'so']\n",
      "['', 'waning']\n",
      "['', 'we', 'actually']\n",
      "['', 'met', 'for', 'the', 'very', 'first', 'time']\n",
      "['', 'the', 'hunt', 'for', 'an', 'appetite', 'to', 'me']\n",
      "['', 'just', 'the', 'feeling', 'the', 'intense', 'feelings', 'and', 'scientists', 'knew']\n",
      "['', 'the', 'to', 'have', 'lines']\n",
      "['', 'and', 'remember', 'theres', 'like', 'these', 'two', 'guys', 'sitting', 'next', 'to', 'our', 'table', 'totally', 'eavesdropping', 'as', 'well', 'and', 'even', 'like', 'and', 'this', 'is', 'chair', 'overstated', 'hear', 'us']\n",
      "['', 'amber', 'thinking', 'this', 'river', 'psychedelic', 'era', 'of', 'my', 'sister']\n",
      "['', 'i', 'grew', 'up', 'thinking', 'at', 'to', 'that', 'clinic', 'my', 'mother', 'in', 'time', 'that', 'you']\n",
      "['', 'theyre', 'embarrassed', 'we', 'get', 'into', 'the', 'insane', 'tell', 'you', 'humphrey', 'got', 'to', 'get']\n",
      "['', 'and', 'of', 'course']\n",
      "['', 'the', 'smile']\n",
      "['', 'then', 'the', 'layer']\n",
      "['', 'chains', 'curved']\n",
      "['', 'i', 'am', 'beginning', 'in', 'the', 'car', 'loans']\n",
      "['', 'my', 'husband']\n",
      "['', 'said', 'i']\n",
      "['', 'how', 'this', 'is', 'turned', 'on']\n",
      "['', 'and', 'as', 'thinking']\n",
      "['', 'had', 'this', 'will', 'be', 'true']\n",
      "['', 'i', 'just', 'look', 'forward', 'to']\n",
      "['', 'learning', 'how', 'to', 'be', 'in']\n",
      "['', 'this', 'is', 'to', 'harry']\n",
      "['', 'its', 'its', 'a', 'weird', 'thing', 'to', 'do', 'to', 'it', 'what', 'is', 'it', 'that']\n",
      "['', 'its', 'something', 'we', 'both', 'have', 'learning', 'to', 'deal', 'a', 'leading', 'get', 'to', 'grow']\n",
      "['', 'together']\n",
      "['', 'but', 'we', 'will', 'get', 'to', 'grow', 'old', 'together', 'and', 'hang', 'on', 'to', 'that', 'alexander', 'sanchez', 'and', 'her', 'sister', 'corny', 'making', 'it', 'their', 'story', 'core', 'interview', 'will', 'be', 'our', 'guide', 'to', 'the', 'american', 'folk', 'like', 'center', 'at', 'the', 'likeliest', 'connors', 'and', 'laugh']\n"
     ]
    }
   ],
   "source": [
    "all_token = []\n",
    "all_token_alltype = []\n",
    "all_sumtoken = []\n",
    "all_gensim = []\n",
    "N = 419 ##304 NPR, 96 conv, 419 story\n",
    "path = 'Data/NPR_story/'\n",
    "#for i in tqdm(range(N)):\n",
    "info_dic = {}\n",
    "for i in range(1):\n",
    "    test_text = []\n",
    "    test_sumtext   = []\n",
    "    info_text = []\n",
    "    test_scores = []\n",
    "    test_lemlength = []\n",
    "    test_length = []\n",
    "    test_timelength = []\n",
    "    test_maxvolume = []\n",
    "    test_avevolume = []\n",
    "    with open(path + str(i) + '_trans.txt', 'r') as myfile:\n",
    "        test_text = cleanText(myfile.read()).split(\".\")\n",
    "    with open(path + str(i) + '.txt', 'r') as myfile:\n",
    "        test_sumtext = cleanText(myfile.read()).split(\".\")[0]\n",
    "   \n",
    "    for j in range(len(test_text)):\n",
    "        info_text.append(mydic[path + str(i) + '_' + str(j) + '.wav'])\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'] = {}\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'].update({'text': test_text[j]})\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'].update({'sumtext': test_sumtext})\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'].update({'sumtext': test_sumtext})\n",
    "    #print(test_sumtext[-1])\n",
    "    #print(info_text[-1])\n",
    "    \n",
    "    ## summ tocken\n",
    "    all_sumtoken.append(lemming(nlp(test_sumtext), doalpha=True, dostop=True, keeptype=[]))\n",
    "    #print(all_sumtoken[-1])\n",
    "    \n",
    "    for j in range(len(test_text)):\n",
    "        temp_text = test_text[j]\n",
    "        print(temp_text.split(\" \"))\n",
    "        test_token = lemming(nlp(\"\".join(temp_text)), doalpha=True, dostop=True, keeptype=[])\n",
    "        test_scores.append(calc_score(test_token, all_sumtoken[-1]))\n",
    "        info_dic[path + str(i) + '_' + str(j) + '.wav'].update({'score': calc_score(test_token, all_sumtoken[-1])})\n",
    "        test_lemlength.append(len(test_token))\n",
    "        test_length.append(len(temp_text.split(\" \")))\n",
    "        test_timelength.append(info_text[j]['duration'])\n",
    "        test_maxvolume.append(info_text[j]['max_dBFS'])\n",
    "        test_avevolume.append(info_text[j]['dBFS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sumtext</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_0.wav</th>\n",
       "      <td>0</td>\n",
       "      <td>after a dna test revealed that they were half ...</td>\n",
       "      <td>the uh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_1.wav</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>after a dna test revealed that they were half ...</td>\n",
       "      <td>then its friday so its time for storing or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_10.wav</th>\n",
       "      <td>1</td>\n",
       "      <td>after a dna test revealed that they were half ...</td>\n",
       "      <td>but yet half sister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_11.wav</th>\n",
       "      <td>0</td>\n",
       "      <td>after a dna test revealed that they were half ...</td>\n",
       "      <td>alexander sand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_12.wav</th>\n",
       "      <td>0</td>\n",
       "      <td>after a dna test revealed that they were half ...</td>\n",
       "      <td>simon says you know as again chance to you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            score  \\\n",
       "Data/NPR_story/0_0.wav          0   \n",
       "Data/NPR_story/0_1.wav   0.333333   \n",
       "Data/NPR_story/0_10.wav         1   \n",
       "Data/NPR_story/0_11.wav         0   \n",
       "Data/NPR_story/0_12.wav         0   \n",
       "\n",
       "                                                                   sumtext  \\\n",
       "Data/NPR_story/0_0.wav   after a dna test revealed that they were half ...   \n",
       "Data/NPR_story/0_1.wav   after a dna test revealed that they were half ...   \n",
       "Data/NPR_story/0_10.wav  after a dna test revealed that they were half ...   \n",
       "Data/NPR_story/0_11.wav  after a dna test revealed that they were half ...   \n",
       "Data/NPR_story/0_12.wav  after a dna test revealed that they were half ...   \n",
       "\n",
       "                                                                text  \n",
       "Data/NPR_story/0_0.wav                                        the uh  \n",
       "Data/NPR_story/0_1.wav    then its friday so its time for storing or  \n",
       "Data/NPR_story/0_10.wav                          but yet half sister  \n",
       "Data/NPR_story/0_11.wav                               alexander sand  \n",
       "Data/NPR_story/0_12.wav   simon says you know as again chance to you  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dic = pd.DataFrame(info_dic).transpose()\n",
    "temp_dic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM8glKXP/D86V2hJrWEZwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a27957898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "#plt.scatter(test_maxvolume, test_scores)\n",
    "plt.scatter(np.array(test_maxvolume)/np.array(test_avevolume), test_scores)\n",
    "#plt.scatter(np.array(test_timelength)/np.array(test_length), test_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and as thinking'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[np.argmax(np.array(test_maxvolume)/np.array(test_avevolume))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(alldic).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dBFS</th>\n",
       "      <th>duration</th>\n",
       "      <th>max</th>\n",
       "      <th>max_dBFS</th>\n",
       "      <th>rms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0.wav</th>\n",
       "      <td>-20.927293</td>\n",
       "      <td>148.897959</td>\n",
       "      <td>28676.0</td>\n",
       "      <td>-1.158627</td>\n",
       "      <td>2945.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_0.wav</th>\n",
       "      <td>-21.294781</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>11625.0</td>\n",
       "      <td>-9.001139</td>\n",
       "      <td>2823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_1.wav</th>\n",
       "      <td>-22.188420</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>27252.0</td>\n",
       "      <td>-1.601031</td>\n",
       "      <td>2547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_10.wav</th>\n",
       "      <td>-23.579804</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>17437.0</td>\n",
       "      <td>-5.479563</td>\n",
       "      <td>2170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/NPR_story/0_11.wav</th>\n",
       "      <td>-24.240757</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>11787.0</td>\n",
       "      <td>-8.880933</td>\n",
       "      <td>2011.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              dBFS    duration      max  max_dBFS     rms\n",
       "Data/NPR_story/0.wav    -20.927293  148.897959  28676.0 -1.158627  2945.0\n",
       "Data/NPR_story/0_0.wav  -21.294781    1.530000  11625.0 -9.001139  2823.0\n",
       "Data/NPR_story/0_1.wav  -22.188420    3.400000  27252.0 -1.601031  2547.0\n",
       "Data/NPR_story/0_10.wav -23.579804    1.080000  17437.0 -5.479563  2170.0\n",
       "Data/NPR_story/0_11.wav -24.240757    1.160000  11787.0 -8.880933  2011.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'info_1.pkl', 'rb') as f:\n",
    "    tempdic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data/NPR_story/1_0.wav': \"in in this friday which is where we're from strike or and today we have a man with an awkward last night hi mrs allen fuchs\",\n",
       " 'Data/NPR_story/1_1.wav': 'my last name is build a few kate has his son of russian immigrants robbed of the united states getting a lot of taunting',\n",
       " 'Data/NPR_story/1_10.wav': 'they were united in making fond of the last name fuchs',\n",
       " 'Data/NPR_story/1_11.wav': 'that pembleton people together',\n",
       " 'Data/NPR_story/1_12.wav': \"the argument the kids on the lowest soulful wrong they don't want me sit at their lunch that\",\n",
       " 'Data/NPR_story/1_13.wav': 'so i would go to the library',\n",
       " 'Data/NPR_story/1_14.wav': 'because i was alone',\n",
       " 'Data/NPR_story/1_15.wav': \"never wear wouldn't hire holocaust encyclopedia\",\n",
       " 'Data/NPR_story/1_16.wav': 'i reckon eisenhower is godard',\n",
       " 'Data/NPR_story/1_17.wav': 'but i was such a lonely period that they have human contact',\n",
       " 'Data/NPR_story/1_18.wav': \"that's what the result would call nintendo hot war\",\n",
       " 'Data/NPR_story/1_19.wav': 'the house with what the',\n",
       " 'Data/NPR_story/1_2.wav': 'about that last name',\n",
       " 'Data/NPR_story/1_20.wav': \"i remember trying to painfully segue from the commerce is rubbing against contagious and so how's it going your life dickhead yzaguirre\",\n",
       " 'Data/NPR_story/1_21.wav': \"that's the sweet my child\",\n",
       " 'Data/NPR_story/1_22.wav': 'the men around six years old my parents',\n",
       " 'Data/NPR_story/1_23.wav': 'decided to try manger',\n",
       " 'Data/NPR_story/1_24.wav': \"there were like you're pale to be irish\",\n",
       " 'Data/NPR_story/1_25.wav': \"so it's roy's irish name to sort out into a hat\",\n",
       " 'Data/NPR_story/1_26.wav': 'handpicked allison',\n",
       " 'Data/NPR_story/1_27.wav': \"and then i went to school now the cake eyes i'm no longer the f. word\",\n",
       " 'Data/NPR_story/1_28.wav': \"i'm cinematic what would you talking about i didn't buy it for us to be on top\",\n",
       " 'Data/NPR_story/1_29.wav': 'but you know',\n",
       " 'Data/NPR_story/1_3.wav': 'you recently talked about the teasing with his middle school classmates banter kept',\n",
       " 'Data/NPR_story/1_30.wav': 'this past year i decided to go back to fuchs',\n",
       " 'Data/NPR_story/1_31.wav': 'that is going to my head bent on letting the bullies when',\n",
       " 'Data/NPR_story/1_32.wav': \"and i'm robert reading about this\",\n",
       " 'Data/NPR_story/1_33.wav': 'philosophy of broken vase',\n",
       " 'Data/NPR_story/1_34.wav': 'but you go back together',\n",
       " 'Data/NPR_story/1_35.wav': 'can you show proudly',\n",
       " 'Data/NPR_story/1_36.wav': 'because the glue is now part of the art',\n",
       " 'Data/NPR_story/1_37.wav': \"i'm hoping to do something like that it like\",\n",
       " 'Data/NPR_story/1_38.wav': 'dan',\n",
       " 'Data/NPR_story/1_39.wav': 'start going the pieces back',\n",
       " 'Data/NPR_story/1_4.wav': \"no mccoy by mining o's either the f. word dumbo because of acute years or combination that too is that the us walking around with an army of hecklers be\",\n",
       " 'Data/NPR_story/1_40.wav': 'but the end knew that alice us harmless better catch the new york city that covers national guard to live for a thought',\n",
       " 'Data/NPR_story/1_5.wav': \"i still with us can dislike the small ain't haunting that was always there are great but what i remember the first time\",\n",
       " 'Data/NPR_story/1_6.wav': \"it's on it\",\n",
       " 'Data/NPR_story/1_7.wav': 'you can step out on the first',\n",
       " 'Data/NPR_story/1_8.wav': \"you better tell when a gold chain mail about your chest with a puppy anchorman a haircut and ted koppel haircut and i we don't know that i had it been need to stick up for you at that age no wonder what element at the kamikaze mission\",\n",
       " 'Data/NPR_story/1_9.wav': 'now this country is so polarized but kids of all demographic'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-0051d26b8cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    274\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   5494\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5496\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5497\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5498\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   5533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5535\u001b[0;31m             raise ValueError('If using all scalar values, you must pass'\n\u001b[0m\u001b[1;32m   5536\u001b[0m                              ' an index')\n\u001b[1;32m   5537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "df_temp = pd.DataFrame(tempdic)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
