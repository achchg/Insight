{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim, string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # get rid of punctuation\n",
    "    text = text.translate(table)\n",
    "    \n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    \n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def calc_score(ref, comp, debug=False):\n",
    "    '''gives the number of items in ref that is also found in comp'''\n",
    "    ## check if it is a list of strings\n",
    "    if not isinstance(ref, list):\n",
    "        ref = str(ref).split()\n",
    "    if not isinstance(comp, list):\n",
    "        comp = str(comp).split()\n",
    "        \n",
    "    s_ref = set(ref)\n",
    "    s_comp = set(comp)\n",
    "    s_inter = s_comp.intersection(s_ref)\n",
    "    if debug:\n",
    "        print(s_ref)\n",
    "        print(s_comp)\n",
    "        print(s_inter)\n",
    "    return len(s_inter)/len(s_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\", \"\\n\", \"\", \" \", \"\\n\\n\", \"npr\"]\n",
    "\n",
    "def lemming(data, keeptype=[\"NOUN\", \"PROPN\", \"NUM\", \"ADJ\", \"ADV\"], doalpha=True):\n",
    "    tokens = []\n",
    "    for tok in data:\n",
    "        \n",
    "        # stoplist the tokens\n",
    "        if tok not in STOPLIST:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # stoplist symbols\n",
    "        if tok not in SYMBOLS:\n",
    "            pass\n",
    "        else: \n",
    "            continue\n",
    "        \n",
    "        ##check if the token is alpha\n",
    "        if doalpha:\n",
    "            if tok.is_alpha:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        ##check if the token is stopword\n",
    "        if not tok.is_stop:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        ##check if the token is noun\n",
    "        if len(keeptype) > 1:\n",
    "            if tok.pos_ in keeptype:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # lemmatize\n",
    "        if tok.lemma_ != \"-PRON-\" :\n",
    "            tokens.append(tok.lemma_.lower().strip())\n",
    "        else:\n",
    "            tokens.append(tok.lower_)\n",
    "    \n",
    "    # remove large strings of whitespace\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=1,                        # minimum reqd occurences of a word \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token = []\n",
    "all_token_alltype = []\n",
    "all_sumtoken = []\n",
    "N = 300\n",
    "for i in tqdm(range(N)):\n",
    "    test_text = []\n",
    "    test_sumtext   = []\n",
    "    with open('Data/NPR/' + str(i) + '_trans.txt', 'r') as myfile:\n",
    "        test_text.append(cleanText(myfile.read()))\n",
    "    with open('Data/NPR/' + str(i) + '.txt', 'r') as myfile:\n",
    "        test_sumtext.append(cleanText(myfile.read()))\n",
    "    ## calculate the maximum score\n",
    "    nlp_test_text = nlp(\"\".join(test_text))\n",
    "    all_token.append(lemming(nlp_test_text))\n",
    "    all_token_alltype.append(lemming(nlp_test_text, doalpha=False, keeptype=[]))\n",
    "    all_sumtoken.append(lemming(nlp(\"\".join(test_sumtext)), doalpha=False, keeptype=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a28ddd9d192d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_rate\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m## calculate the maximum score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "base_rate = []\n",
    "my_rate  = []\n",
    "for i in tqdm(range(N)):\n",
    "    ## calculate the maximum score\n",
    "    test_token = all_token[i]\n",
    "    test_token_alltype = all_token_alltype[i]\n",
    "    test_sumtoken = all_sumtoken[i]\n",
    "    best_score = calc_score(ref=test_sumtoken, comp=test_token, debug=False)\n",
    "    #print(\"best score\", best_score)\n",
    "\n",
    "    length = len(test_sumtoken)\n",
    "    common_words_alltype = [w[0] for w in Counter(test_token_alltype).most_common(length)]\n",
    "    #print(\" \".join(common_words))\n",
    "    base_score =  calc_score(ref=test_sumtoken, comp=common_words_alltype, debug=False)\n",
    "    #print(\"baseline score\", base_score)\n",
    "\n",
    "#     # Predict the topic\n",
    "#     topic = predict_topic(text = [\" \".join(test_token)], nwords=length)\n",
    "#     #print(\" \".join(topic))\n",
    "\n",
    "    topic = [w[0] for w in Counter(test_token).most_common(length)]\n",
    "    my_score = calc_score(ref=test_sumtoken, comp=topic, debug=False)\n",
    "    #print(\"my score\", my_score)\n",
    "    #print(\" \".join(test_sumtoken))\n",
    "    base_rate.append(base_score/best_score)\n",
    "    my_rate.append(my_score/best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
