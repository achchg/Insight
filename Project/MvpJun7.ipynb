{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating first MVP product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations for extra packages: \n",
    "- !pip install SpeechRecognition\n",
    "- !pip install PocketSphinx\n",
    "- !brew install portaudio\n",
    "- !pip install pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources:\n",
    "\n",
    "- __[NYT daily from podbay](http://podbay.fm/show/1200361736/e/1527587699?autostart=1)__; \n",
    "- NYT daily: https://itunes.apple.com/us/podcast/the-daily/id1200361736?mt=2&ign-mpt=uo%3D4\n",
    "- Open Audio Source: http://www.openslr.org/resources.php\n",
    "- Economist: https://www.economist.com/audio-edition/2018-06-02\n",
    "- Economist text: https://xmuplus.github.io/\n",
    "- NPR: https://www.npr.org/programs/morning-edition/\n",
    "- NPR sunday: https://www.npr.org/programs/weekend-edition-sunday/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition\n",
    "\n",
    "- Real python: https://realpython.com/python-speech-recognition/#working-with-audio-files\n",
    "- Speech Recognition: https://github.com/Uberi/speech_recognition#readme\n",
    "- Speech Recognition: https://pypi.org/project/SpeechRecognition/3.2.0/\n",
    "- Pocket Sphinx: https://pypi.org/project/pocketsphinx/\n",
    "- Pydub: https://github.com/jiaaro/pydub/blob/master/API.markdown\n",
    "- Audio processing in python: http://myinspirationinformation.com/uncategorized/audio-signals-in-python/\n",
    "- Speaker Recognition: https://github.com/orchidas/Speaker-Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "- Insight: https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n",
    "- Intro/resource: https://github.com/icoxfog417/awesome-text-summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "### Import basic libraries\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for speech to text\n",
    "import speech_recognition as sr\n",
    "\n",
    "\n",
    "## for audio processing and analytical study\n",
    "from pydub import AudioSegment ##for audio spliting\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "## for audio collection\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "## for text recognition\n",
    "import nltk\n",
    "\n",
    "## for general studies\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import fft as fft\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    " \n",
    "%matplotlib inline\n",
    "# !ls Data/EC_06022018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intitialize a recognizer\n",
    "r_live = sr.Recognizer()\n",
    "r_rec  = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain audio from the microphone\n",
    "# with sr.Microphone() as source:\n",
    "#     print(\"Say something!\")\n",
    "#     audio = r_live.listen(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "### Study WAV sound files\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wav file path\n",
    "## Text link: https://www.economist.com/science-and-technology/2018/06/02/a-rocket-that-devours-itself\n",
    "f_wav_man = 'Data/EC_06092018/060980.wav' ##Male\n",
    "## Text link: https://www.economist.com/science-and-technology/2018/06/02/hiv-volunteers-are-bequeathing-their-organs-to-a-new-project\n",
    "f_wav_woman = 'Data/EC_06092018/060979.wav' #\n",
    "f_wav = f_wav_woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the length of audio file\n",
    "sound = AudioSegment.from_file(f_wav)\n",
    "print(sound.duration_seconds)\n",
    "print(\"DBFS\", sound.dBFS, \"Max DBFS\", sound.max_dBFS)\n",
    "print(\"RMS\", sound.rms)\n",
    "print(\"Frame rate\", sound.frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the raw sound data as array\n",
    "r_np = sound.get_array_of_samples()\n",
    "\n",
    "##inspect time vs Amplitude\n",
    "plt.clf()\n",
    "plt.title(f_wav)\n",
    "plt.plot(np.linspace(0, len(r_np)/sound.frame_rate, len(r_np)), r_np, linewidth=0.02, alpha=0.8)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frequency analysis, if needed\n",
    "fourier=fft.fft(r_np)\n",
    "plt.clf()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fourier, color='#ff7f00')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "n = len(r_np)\n",
    "fourier = fourier[0:int(n/2)]\n",
    "\n",
    "# scale by the number of points so that the magnitude does not depend on the length\n",
    "fourier = fourier / float(n)\n",
    "fourier = abs(fourier)\n",
    "fourier = fourier ** 2\n",
    "\n",
    "# rebase 0 as the minimal voice\n",
    "vol =  10*np.log10(fourier) - 10*np.log10(min(fourier))\n",
    "\n",
    "#calculate the frequency at each point in Hz\n",
    "freqArray = np.arange(0, (n/2), 1.0) * (sound.frame_rate*1.0/n);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(freqArray/1000, vol, color='#ff7f00', linewidth=0.02)\n",
    "plt.xlabel('Frequency (kHz)')\n",
    "plt.ylabel('Power (dB)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "### Sound file decomposition\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sound in 10-second slices and export\n",
    "start_time = time.time()\n",
    "sound_file = AudioSegment.from_wav(f_wav)\n",
    "\n",
    "for i, chunk in enumerate(sound_file[::10 * 1000]):\n",
    "  n_files = i\n",
    "  with open(\"chunk-%s.wav\" % i, \"wb\") as f:\n",
    "    chunk.export(f, format=\"wav\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## silence split, if necessary\n",
    "# start_time = time.time()\n",
    "# sound_file = AudioSegment.from_wav(f_wav)\n",
    "# audio_chunks = split_on_silence(sound_file, \n",
    "#     # must be silent for at least half a second\n",
    "#     min_silence_len=1000,\n",
    "#     # consider it silent if quieter than -16 dBFS\n",
    "#     silence_thresh=-16)\n",
    "\n",
    "# print(len(audio_chunks))\n",
    "# ## split into chunks\n",
    "# for i, chunk in enumerate(audio_chunks):\n",
    "#     out_file = \"chunk{0}.wav\".format(i)\n",
    "#     print(\"exporting\", out_file)\n",
    "#     chunk.export(out_file, format=\"wav\")\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "### Text recognition\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in tqdm(range(n_files + 1)):\n",
    "    print(i)\n",
    "    test = sr.AudioFile(\"chunk-%s.wav\" % i)\n",
    "    with test as source:\n",
    "        test_au = r_rec.record(source)\n",
    "        fulltext += r_rec.recognize_sphinx(test_au, language='en-US')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtext = \"MUCH of the medical research conducted on HIV, the virus that causes AIDS, looks at patients’ blood. This is no surprise. Blood is both easy to collect and easy to preserve. But HIV is not confined to the bloodstreams of those infected by it. It is found in almost all of their bodily tissues. In the view of Davey Smith, a virologist at the University of California, San Diego (UCSD), focusing only on the metaphorical “trees” of the blood is therefore a mistake. It misses the “forest” of the other organs. Inspired by similar programmes in cancer research, Dr Smith therefore set up, in July 2017, a project called “Last Gift”. This seeks HIV-positive volunteers who are terminally ill for some other reason and asks them to bequeath their tissues for cryogenic preservation and subsequent study. So far, five people have signed up, two of whom have died. Dr Smith hopes for 20 more over the next four years.The crux of Last Gift’s operation is speed, because HIV’s genes and proteins start to degrade within four hours of a patient’s death. An autopsy team is therefore always on call to attend a volunteer’s deathbed, collect samples from his organs and bring them back to a laboratory at UCSD for cryogenic preservation. The team take specimens of brain, spinal cord, lungs, heart, kidneys, liver, spleen, muscle, bone marrow, adrenal glands, thyroid, lymph nodes, genital-tract tissues, foreskin and intestine.That HIV can hide in such solid tissues has been known for years. It is a retrovirus, meaning that it integrates its genes into its host’s DNA. Once integrated in this way it can remain dormant indefinitely. The resulting reservoirs are the main barrier to eradicating it from someone’s system. Existing drugs control viral replication, but cannot affect dormant, integrated viral genes. If someone stops taking those drugs it requires only a small leak from one of the reservoirs to bring the infection roaring back. Dormancy, moreover, makes HIV invisible to the immune system. Understanding viral dormancy in solid tissues is thus important.Even though they have only two sets of tissues to work with at the moment, Dr Smith and his colleagues have already made discoveries. They have, for example, recorded surprisingly high levels of live (as opposed to dormant) virus in the brain, the spleen and the liver. They have also documented disparities in the levels of live virus within and between these organs.They are especially interested in epigenetic modifications of cells taken from their volunteers’ organs. Such modifications, which serve to regulate the activity of genes, are chemical alterations of a cell’s DNA and of the proteins in which that DNA is packed. The team hope to spot epigenetic patterns that will both give away those cells which are infected and help explain how HIV genes in a cell’s nucleus are activated and deactivated.Another area of specific concern is how HIV replicates in the gut. Most new particles of the virus are produced in immune-system cells called T-lymphocytes. And most of the human immune system resides in the intestines, where it deals with pathogens ingested by mouth that have not succumbed to the acidity of the stomach. Understanding what is going on in the intestines is thus crucial to understanding the way the infection sustains itself once it has become established.The need for speed means Last Gift is, at the moment, necessarily confined to volunteers living in, or close to, San Diego. But Dr Smith is hopeful that his method will be replicated elsewhere. His team are already sharing data with researchers at the University of California, San Francisco, who might one day start their own version of the operation. And scientists from three other American universities, and also the National Institutes of Health, have expressed interest in partnerships. That is to be welcomed. Any true cure for HIV infection will involve flushing the virus out of its solid-tissue hidey-holes. Knowing what is really going on in those hidey-holes is therefore essential.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Text exploration\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = tokenizer.tokenize(realtext)\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "print(tokens[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new list to hold the lowercased words\n",
    "words = []\n",
    "\n",
    "# Looping through the tokens and make them lower case\n",
    "for word in tokens:\n",
    "    words.append(word.lower())\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the English stop words from nltk\n",
    "# first time: nltk.download('stopwords')\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "print(sw[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new list to hold Moby Dick with No Stop words\n",
    "words_ns = []\n",
    "\n",
    "# Appending to words_ns all words that are in words but not in sw\n",
    "for word in words:\n",
    "    if word not in sw:\n",
    "        words_ns.append(word)\n",
    "\n",
    "# Printing the first 5 words_ns to check that stop words are gone\n",
    "print(words_ns[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the word frequency distribution\n",
    "freqdist = nltk.probability.FreqDist(words_ns)\n",
    "\n",
    "# Plotting the word frequency distribution\n",
    "freqdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Live Audio import\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## interactive\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_live(Name=\"output\", Length=100, Nout=1, Running=False):\n",
    "    '''this function records audio, length in seconds'''\n",
    "    start_time = time.time()\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    RATE = 32000\n",
    "    RECORD_SECONDS = 10 ## fixed 10 sec pieces\n",
    "    CHANNELS = 1  ##2 for stereo\n",
    "    CHUNK = 1024\n",
    "\n",
    "    ## initialize pyaudio\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    N_rec = Length/RECORD_SECONDS\n",
    "    n_rec = 0\n",
    "\n",
    "    #keypressed = input('Press q to quit: ')\n",
    "    while n_rec < N_rec:\n",
    "        stream = p.open(format=FORMAT,\n",
    "                            channels=CHANNELS,\n",
    "                            rate=RATE,\n",
    "                            input=True,\n",
    "                            frames_per_buffer=CHUNK)\n",
    "        print(\"* recording: \" + str(n_rec))\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "\n",
    "        ## initialize output waveform\n",
    "        wf = wave.open(\"Data/Live/\" + Name + \"_\" + str(n_rec) + \".wav\", 'wb')\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        n_rec += 1\n",
    "        \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf.close()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_live(Length=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls Data/Live/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "test = sr.AudioFile(\"Data/Live/\" + \"output_1\" + \".wav\")\n",
    "with test as source:\n",
    "    test_au = r_rec.record(source)\n",
    "print(\"Sphinx:\", r_rec.recognize_sphinx(test_au, language='en-US', keyword_entries=None, grammar=None, show_all=False))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
